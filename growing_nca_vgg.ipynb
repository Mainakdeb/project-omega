{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiABXwz5ppYS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms.functional as F\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "import imageio\n",
        "\n",
        "from base64 import b64encode\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "from IPython.display import  HTML, clear_output\n",
        "import matplotlib.pylab as pl\n",
        "\n",
        "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
        "import moviepy.editor as mvp\n",
        "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SfRTwOM4p2D8"
      },
      "outputs": [],
      "source": [
        "vgg16 = models.vgg16(pretrained=True).features\n",
        "vgg16_model = models.vgg16(pretrained=True).cuda()\n",
        "\n",
        "def calc_styles(imgs):\n",
        "  style_layers = [1, 6, 11, 18, 25]  \n",
        "  mean = torch.tensor([0.485, 0.456, 0.406])[:,None,None]\n",
        "  std = torch.tensor([0.229, 0.224, 0.225])[:,None,None]\n",
        "  x = (imgs-mean) / std\n",
        "  grams = []\n",
        "  for i, layer in enumerate(vgg16[:max(style_layers)+1]):\n",
        "    x = layer(x)\n",
        "    if i in style_layers:\n",
        "      h, w = x.shape[-2:]\n",
        "      y = x.clone()  # workaround for pytorch in-place modification bug(?)\n",
        "      gram = torch.einsum('bchw, bdhw -> bcd', y, y) / (h*w)\n",
        "      grams.append(gram)\n",
        "  return grams\n",
        "\n",
        "def style_loss(grams_x, grams_y):\n",
        "  loss = 0.0\n",
        "  for x, y in zip(grams_x, grams_y):\n",
        "    loss = loss + (x-y).square().mean()\n",
        "  return loss\n",
        "\n",
        "def to_nchw(img):\n",
        "  img = torch.as_tensor(img)\n",
        "  if len(img.shape) == 3:\n",
        "    img = img[None,...]\n",
        "  return img.permute(0, 3, 1, 2)\n",
        "\n",
        "def class_loss(imgs, class_idx):\n",
        "    resized_i = F.resize(imgs, size=(224, 224))\n",
        "    o = vgg16_model(resized_i)\n",
        "    r = o[:, class_idx]\n",
        "    return(-r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CrPeY7G0qSiP"
      },
      "outputs": [],
      "source": [
        "class CAModel(nn.Module):\n",
        "    def __init__(self, n_channels=16, hidden_channels=128, fire_rate=0.5, device=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fire_rate = 0.5\n",
        "        self.n_channels = n_channels\n",
        "        self.device = device or torch.device(\"cpu\")\n",
        "\n",
        "        # Perceive step\n",
        "        sobel_filter_ = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
        "        scalar = 8.0\n",
        "\n",
        "        sobel_filter_x = sobel_filter_ / scalar\n",
        "        sobel_filter_y = sobel_filter_.t() / scalar\n",
        "        identity_filter = torch.tensor(\n",
        "                [\n",
        "                    [0, 0, 0],\n",
        "                    [0, 1, 0],\n",
        "                    [0, 0, 0],\n",
        "                ],\n",
        "                dtype=torch.float32,\n",
        "        )\n",
        "        filters = torch.stack(\n",
        "                [identity_filter, sobel_filter_x, sobel_filter_y]\n",
        "        )  # (3, 3, 3)\n",
        "        filters = filters.repeat((n_channels, 1, 1))  # (3 * n_channels, 3, 3)\n",
        "        self.filters = filters[:, None, ...].to(\n",
        "                self.device\n",
        "        )  # (3 * n_channels, 1, 3, 3)\n",
        "\n",
        "        # Update step\n",
        "        self.update_module = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    3 * n_channels,\n",
        "                    hidden_channels,\n",
        "                    kernel_size=1,  # (1, 1)\n",
        "                ),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(\n",
        "                    hidden_channels,\n",
        "                    n_channels,\n",
        "                    kernel_size=1,\n",
        "                    bias=False,\n",
        "                ),\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.update_module[2].weight.zero_()\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def perceive(self, x):\n",
        "        return nn.functional.conv2d(x, self.filters, padding=1, groups=self.n_channels)\n",
        "\n",
        "    def update(self, x):\n",
        "        return self.update_module(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def stochastic_update(x, fire_rate):\n",
        "        device = x.device\n",
        "\n",
        "        mask = (torch.rand(x[:, :1, :, :].shape) <= fire_rate).to(device, torch.float32)\n",
        "        return x * mask  # broadcasted over all channels\n",
        "\n",
        "    @staticmethod\n",
        "    def get_living_mask(x):\n",
        "        return (\n",
        "            nn.functional.max_pool2d(\n",
        "                x[:, 3:4, :, :], kernel_size=3, stride=1, padding=1\n",
        "            )\n",
        "            > 0.1\n",
        "        )\n",
        "    def seed(self, n, sz=128):\n",
        "      x = torch.zeros((n, self.n_channels, sz, sz), dtype=torch.float32)\n",
        "      x[:, 3:, sz // 2, sz // 2] = 1\n",
        "      return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_life_mask = self.get_living_mask(x)\n",
        "\n",
        "        y = self.perceive(x)\n",
        "        dx = self.update(y)\n",
        "        dx = self.stochastic_update(dx, fire_rate=self.fire_rate)\n",
        "\n",
        "        x = x + dx\n",
        "\n",
        "        post_life_mask = self.get_living_mask(x)\n",
        "        life_mask = (pre_life_mask & post_life_mask).to(torch.float32)\n",
        "\n",
        "        return x * life_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sEk30PjyqShM"
      },
      "outputs": [],
      "source": [
        "def to_rgb(img_rgba):\n",
        "    rgb, a = img_rgba[:, :3, ...], torch.clamp(img_rgba[:, 3:4, ...], 0, 1)\n",
        "    return torch.clamp(1.0 - a + rgb, 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WlfM4kMqqSgH"
      },
      "outputs": [],
      "source": [
        "ca_model = CAModel(device=device)\n",
        "ca_model = ca_model.to(device)\n",
        "optimizer = torch.optim.Adam(ca_model.parameters(), lr=2e-3)\n",
        "lr_sched = torch.optim.lr_scheduler.MultiStepLR(optimizer, [200,700,800,900], 0.4)\n",
        "loss_log = []\n",
        "\n",
        "# Pool initialization\n",
        "with torch.no_grad():\n",
        "  pool = ca_model.seed(n=128, sz=128).to(device)\n",
        "\n",
        "batch_size=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5WBU1gejr6aw"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "\n",
        "for i in range(1000):\n",
        "  with torch.no_grad():\n",
        "    batch_idx = np.random.choice(len(pool), batch_size, replace=False)\n",
        "    x = pool[batch_idx]\n",
        "    if i%8 == 0:\n",
        "      x[:1] = ca_model.seed(1)\n",
        "  step_n = np.random.randint(32, 96)\n",
        "  x = torch.utils.checkpoint.checkpoint_sequential([ca_model]*step_n, 16, x)\n",
        "  imgs = to_rgb(x)\n",
        "  overflow_loss = (x-x.clamp(-1.0, 1.0)).abs().sum()\n",
        "  loss = torch.mean(class_loss(imgs=imgs, class_idx=1))+overflow_loss\n",
        "  with torch.no_grad():\n",
        "    loss.backward()\n",
        "    for p in ca_model.parameters():\n",
        "      p.grad /= (p.grad.norm()+1e-8)   # normalize gradients \n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    lr_sched.step()\n",
        "    pool[batch_idx] = x                # update pool\n",
        "    \n",
        "    loss_log.append(loss.item())\n",
        "    if i%5==0:\n",
        "      clear_output(True)\n",
        "      imgs = to_rgb(x[:, :4]).permute([0, 3, 2, 1]).cpu()\n",
        "      f, axarr = plt.subplots(1,4)\n",
        "      axarr[0].plot(loss_log[-50:], alpha=0.8)\n",
        "    #   plt.yscale('log')\n",
        "    #   pl.ylim(np.min(loss_log), loss_log[0])\n",
        "      axarr[1].imshow(imgs[0].cpu().detach().numpy())\n",
        "      axarr[2].imshow(imgs[1].cpu().detach().numpy())\n",
        "      axarr[3].imshow(imgs[2].cpu().detach().numpy())                      \n",
        "    #  plt.imshow(np.hstack(imgs)[-3:])\n",
        "      plt.show()\n",
        "    if i%5 == 0:\n",
        "      print('\\rstep_n:', len(loss_log),\n",
        "        ' loss:', loss.item(), \n",
        "        ' overflow loss: ', overflow_loss.item(),\n",
        "        ' lr:', lr_sched.get_lr()[0], end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm8_uTlXsEa_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "growing_nca_vgg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}